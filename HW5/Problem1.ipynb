{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPIoqCaSe+OVcnxRhn3Pifg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSalmon13/GenAI/blob/main/HW5/Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Name: David Schwartzman\n",
        "# Date: 11/18/2025\n",
        "# Description: In this model I will build a LSTM model that will take 3 books as an input\n",
        "#              and by studying the text it will generate similar sentences as in the books"
      ],
      "metadata": {
        "id": "xL5ksXtjczk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TB6c3KeiVoB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Data Collection from Project Gutenberg\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/1041/1041-0.txt\",  # Hamlet\n",
        "    \"https://www.gutenberg.org/files/152/152-0.txt\",   # Macbeth\n",
        "    \"https://www.gutenberg.org/files/1112/1112-0.txt\"  # Othello\n",
        "]\n",
        "\n",
        "# Initialize an empty string to hold all text\n",
        "all_text = \"\"\n",
        "\n",
        "# Download each text file and append to all_text\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "    all_text += text + \"\\n\\n\"  # Separate texts by newlines\n",
        "\n",
        "# Save combined text to a single file\n",
        "with open(\"combined_shakespeare.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(all_text)\n"
      ],
      "metadata": {
        "id": "7WZpHkgjiX7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Text Preprocessing (Cleaning, Tokenization)\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters, special symbols, and extra spaces\n",
        "    text = re.sub(r\"\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\r\", \" \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Keep only alphabets and spaces\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "# Clean the collected text\n",
        "cleaned_text = clean_text(all_text)\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "total_words = len(tokenizer.word_index) + 1  # Adding 1 for the padding token\n",
        "\n",
        "# Prepare the data for training\n",
        "input_sequences = []\n",
        "output_words = []\n",
        "\n",
        "# Create input-output sequences for training\n",
        "sequence_length = 50"
      ],
      "metadata": {
        "id": "-edxqjk2ibNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the first 10 token:word mappings\n",
        "token_word_mappings = list(tokenizer.word_index.items())[:10]\n",
        "print(\"Token: Word Mappings (First 10):\")\n",
        "for token, word in token_word_mappings:\n",
        "    print(f\"{token}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmlgjiafXud",
        "outputId": "3ee642ba-63a8-4c07-c90d-5438bf106ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Word Mappings (First 10):\n",
            "and: 1\n",
            "the: 2\n",
            "to: 3\n",
            "i: 4\n",
            "of: 5\n",
            "my: 6\n",
            "that: 7\n",
            "a: 8\n",
            "in: 9\n",
            "thou: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\", input_sequences[0])\n",
        "print(\"Output:\", output_words[0])\n"
      ],
      "metadata": {
        "id": "ZkBkaUX2g_rB",
        "outputId": "e3f14ff4-269d-45c7-941d-0d96388a29e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [884, 5, 2, 605, 537, 885, 2, 2695, 28, 2696, 2697, 4, 36, 755, 1315, 84, 390, 1047, 7, 1765, 296, 673, 179, 337, 164, 19, 24, 2, 1766, 66, 28, 69, 1316, 25, 360, 1317, 179, 421, 25, 538, 19, 10, 1767, 3, 123, 184, 322, 75, 2698, 12]\n",
            "Output: 1048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(sequence_length, len(cleaned_text.split())):\n",
        "    sequence = cleaned_text.split()[i-sequence_length:i]\n",
        "    input_sequences.append(tokenizer.texts_to_sequences([sequence])[0])\n",
        "    output_words.append(tokenizer.texts_to_sequences([cleaned_text.split()[i]])[0][0])\n",
        "\n",
        "# Convert input sequences and output words to numpy arrays\n",
        "X = np.array(input_sequences)\n",
        "y = np.array(output_words)\n",
        "\n",
        "# One-hot encode the output labels\n",
        "y = to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "feOA7_3LinTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size, sequence_length, lstm_units=128, num_layers=1):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, EMBEDDING_DIM, input_length=sequence_length))\n",
        "\n",
        "    # Add LSTM layers\n",
        "    for i in range(num_layers - 1):\n",
        "        model.add(layers.LSTM(lstm_units, return_sequences=True))\n",
        "        #lstm_units *= 2\n",
        "    model.add(layers.LSTM(lstm_units))  # Last LSTM layer without return_sequences\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "mW0O1sQyirry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Training the Model\n",
        "VOCAB_SIZE = total_words\n",
        "MAX_LEN = sequence_length\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Create and train the model\n",
        "lstm_model = create_model(VOCAB_SIZE, MAX_LEN, lstm_units=128, num_layers=3)\n",
        "\n",
        "lstm_model.summary()\n",
        "# Train the model\n",
        "lstm_model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4UCm31apxBG",
        "outputId": "1f4685a3-e4d7-4280-d98e-4ed6343dced8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 100)           644000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50, 128)           117248    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 50, 128)           131584    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6440)              830760    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1855176 (7.08 MB)\n",
            "Trainable params: 1855176 (7.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "676/676 [==============================] - 119s 170ms/step - loss: 7.0612 - accuracy: 0.0254\n",
            "Epoch 2/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 6.7222 - accuracy: 0.0289\n",
            "Epoch 3/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 6.6008 - accuracy: 0.0328\n",
            "Epoch 4/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 6.4679 - accuracy: 0.0362\n",
            "Epoch 5/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 6.3482 - accuracy: 0.0387\n",
            "Epoch 6/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 6.2323 - accuracy: 0.0416\n",
            "Epoch 7/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 6.1126 - accuracy: 0.0463\n",
            "Epoch 8/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 5.9853 - accuracy: 0.0524\n",
            "Epoch 9/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 5.8450 - accuracy: 0.0594\n",
            "Epoch 10/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 5.7046 - accuracy: 0.0653\n",
            "Epoch 11/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 5.5807 - accuracy: 0.0699\n",
            "Epoch 12/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 5.4557 - accuracy: 0.0745\n",
            "Epoch 13/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 5.3367 - accuracy: 0.0792\n",
            "Epoch 14/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 5.2576 - accuracy: 0.0828\n",
            "Epoch 15/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 5.0954 - accuracy: 0.0881\n",
            "Epoch 16/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 4.9703 - accuracy: 0.0931\n",
            "Epoch 17/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 4.8520 - accuracy: 0.1015\n",
            "Epoch 18/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 4.7358 - accuracy: 0.1084\n",
            "Epoch 19/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 4.6222 - accuracy: 0.1156\n",
            "Epoch 20/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 4.5100 - accuracy: 0.1256\n",
            "Epoch 21/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 4.3983 - accuracy: 0.1396\n",
            "Epoch 22/50\n",
            "676/676 [==============================] - 116s 172ms/step - loss: 4.2924 - accuracy: 0.1524\n",
            "Epoch 23/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 4.1914 - accuracy: 0.1669\n",
            "Epoch 24/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 4.0951 - accuracy: 0.1818\n",
            "Epoch 25/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 3.9976 - accuracy: 0.1958\n",
            "Epoch 26/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 3.9137 - accuracy: 0.2091\n",
            "Epoch 27/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 3.8198 - accuracy: 0.2236\n",
            "Epoch 28/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 3.7531 - accuracy: 0.2369\n",
            "Epoch 29/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 3.6574 - accuracy: 0.2502\n",
            "Epoch 30/50\n",
            "676/676 [==============================] - 116s 172ms/step - loss: 3.5781 - accuracy: 0.2609\n",
            "Epoch 31/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 3.5063 - accuracy: 0.2724\n",
            "Epoch 32/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 3.4329 - accuracy: 0.2831\n",
            "Epoch 33/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 3.3642 - accuracy: 0.2955\n",
            "Epoch 34/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 3.3106 - accuracy: 0.3033\n",
            "Epoch 35/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 3.2428 - accuracy: 0.3140\n",
            "Epoch 36/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 3.1800 - accuracy: 0.3256\n",
            "Epoch 37/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 3.1150 - accuracy: 0.3367\n",
            "Epoch 38/50\n",
            "676/676 [==============================] - 116s 172ms/step - loss: 3.0552 - accuracy: 0.3443\n",
            "Epoch 39/50\n",
            "676/676 [==============================] - 115s 170ms/step - loss: 2.9940 - accuracy: 0.3546\n",
            "Epoch 40/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 2.9384 - accuracy: 0.3685\n",
            "Epoch 41/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.8778 - accuracy: 0.3743\n",
            "Epoch 42/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.8221 - accuracy: 0.3878\n",
            "Epoch 43/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.7709 - accuracy: 0.3959\n",
            "Epoch 44/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 2.7159 - accuracy: 0.4055\n",
            "Epoch 45/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.6651 - accuracy: 0.4152\n",
            "Epoch 46/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.6095 - accuracy: 0.4258\n",
            "Epoch 47/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.5611 - accuracy: 0.4352\n",
            "Epoch 48/50\n",
            "676/676 [==============================] - 116s 171ms/step - loss: 2.5143 - accuracy: 0.4440\n",
            "Epoch 49/50\n",
            "676/676 [==============================] - 116s 172ms/step - loss: 2.4612 - accuracy: 0.4559\n",
            "Epoch 50/50\n",
            "676/676 [==============================] - 115s 171ms/step - loss: 2.4163 - accuracy: 0.4627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e83180c6890>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class TextGenerator:\n",
        "    def __init__(self, model, tokenizer, max_len=50):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def generate(self, seed_text, max_tokens=50, temperature=1.0):\n",
        "        input_text = seed_text.lower()\n",
        "\n",
        "        # Tokenize the input text\n",
        "        sequence = self.tokenizer.texts_to_sequences([input_text])[0]\n",
        "\n",
        "        # Ensure the sequence is the correct length (pad or truncate)\n",
        "        sequence = sequence[-self.max_len:]\n",
        "        sequence = pad_sequences([sequence], maxlen=self.max_len, padding='pre', truncating='pre')\n",
        "\n",
        "        for _ in range(max_tokens):\n",
        "            # Predict the next word\n",
        "            prediction = self.model.predict(sequence, verbose=0)\n",
        "            prediction = prediction[0]\n",
        "\n",
        "            # Adjust predictions based on temperature\n",
        "            prediction = np.asarray(prediction).astype(\"float64\")\n",
        "            prediction = np.log(prediction + 1e-7) / temperature\n",
        "            exp_probs = np.exp(prediction)\n",
        "            prediction = exp_probs / np.sum(exp_probs)\n",
        "\n",
        "            # Sample a word from the distribution\n",
        "            next_index = np.random.choice(len(prediction), p=prediction)\n",
        "            next_word = self.tokenizer.index_word[next_index]\n",
        "\n",
        "            # Append the word to the input text\n",
        "            input_text += \" \" + next_word\n",
        "\n",
        "            # Update the sequence with the newly generated word\n",
        "            sequence = self.tokenizer.texts_to_sequences([input_text])[-1]\n",
        "            sequence = sequence[-self.max_len:]\n",
        "            sequence = pad_sequences([sequence], maxlen=self.max_len, padding='pre', truncating='pre')\n",
        "\n",
        "        return input_text\n"
      ],
      "metadata": {
        "id": "aytA4htaifoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the TextGenerator class\n",
        "text_generator = TextGenerator(lstm_model, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# Generate text based on seed prompts\n",
        "seed_prompts = [\n",
        "    \"to be or not to be\",\n",
        "    \"shall I compare thee to a summer's day\",\n",
        "    \"all the world's a stage\"\n",
        "]\n",
        "\n",
        "# Experiment with different temperature settings\n",
        "for prompt in seed_prompts:\n",
        "    print(f\"Seed prompt: {prompt}\")\n",
        "    for temp in [0.1,0.5,1.0]:\n",
        "        print(f\"Temperature: {temp}\")\n",
        "        generated_text = text_generator.generate(prompt, max_tokens=20, temperature=temp)\n",
        "        print(f\"Generated text: {generated_text}\\n\")"
      ],
      "metadata": {
        "id": "pAj_glFMoFS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dee5349-96e1-44c7-e5e9-8016333d4d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed prompt: to be or not to be\n",
            "Temperature: 0.1\n",
            "Generated text: to be or not to be short than all the world rom comes me and go by me i am fel i not the letters of\n",
            "\n",
            "Temperature: 0.5\n",
            "Generated text: to be or not to be bright at recompense shall soone to arme from cupids arrow calme burne distilld with heat must tongues and siren vile\n",
            "\n",
            "Temperature: 1.0\n",
            "Generated text: to be or not to be old without those bound now you on the huswife of the instant grace your white app compare the very eye\n",
            "\n",
            "Seed prompt: shall I compare thee to a summer's day\n",
            "Temperature: 0.1\n",
            "Generated text: shall i compare thee to a summer's day to be peruerse and mayst not proofe for all away speakst thou not iule to moue me to thy wounds\n",
            "\n",
            "Temperature: 0.5\n",
            "Generated text: shall i compare thee to a summer's day to be peruerse and say thee not mercy she mer on the churchyard thrice thing and yet not added like\n",
            "\n",
            "Temperature: 1.0\n",
            "Generated text: shall i compare thee to a summer's day with feather thou knowst i do defy see forth though i haue so danger frier all it iul madam what\n",
            "\n",
            "Seed prompt: all the world's a stage\n",
            "Temperature: 0.1\n",
            "Generated text: all the world's a stage which should not fall about with thee and trim the inundation of thy lips the bath of sepulchres still kissing\n",
            "\n",
            "Temperature: 0.5\n",
            "Generated text: all the world's a stage which should not fears for titans wheeles thus runnst no more and meant robbd that enfeebled thinke life cheeks by\n",
            "\n",
            "Temperature: 1.0\n",
            "Generated text: all the world's a stage all old of glad are men more poore than out those blacke soul then come forerun my defects so truth\n",
            "\n"
          ]
        }
      ]
    }
  ]
}